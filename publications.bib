
@article{yeow_point--interest_2021,
	title = {Point-of-{Interest} ({POI}) {Data} {Validation} {Methods}: {An} {Urban} {Case} {Study}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Point-of-{Interest} ({POI}) {Data} {Validation} {Methods}},
	url = {https://www.mdpi.com/2220-9964/10/11/735},
	doi = {10.3390/ijgi10110735},
	abstract = {Point-of-interest (POI) data from map sources are increasingly used in a wide range of applications, including real estate, land use, and transport planning. However, uncertainties in data quality arise from the fact that some of this data are crowdsourced and proprietary validation workflows lack transparency. Comparing data quality between POI sources without standardized validation metrics is a challenge. This study reviews and implements the available POI validation methods, working towards identifying a set of metrics that is applicable across datasets. Twenty-three validation methods were found and categorized. Most methods evaluated positional accuracy, while logical consistency and usability were the least represented. A subset of nine methods was implemented to assess four real-world POI datasets extracted for a highly urbanized neighborhood in Singapore. The datasets were found to have poor completeness with errors of commission and omission, although spatial errors were reasonably low (\&lt;60 m). Thematic accuracy in names and place types varied. The move towards standardized validation metrics depends on factors such as data availability for intrinsic or extrinsic methods, varying levels of detail across POI datasets, the influence of matching procedures, and the intended application of POI data.},
	language = {en},
	number = {11},
	urldate = {2022-01-20},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Yeow, Lih Wei and Low, Raymond and Tan, Yu Xiang and Cheah, Lynette},
	month = nov,
	year = {2021},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {data quality, point of interest, volunteered geographic information (VGI)},
	pages = {735},
	file = {Full Text PDF:C\:\\Users\\tyxis\\Zotero\\storage\\PYQNFMGN\\Yeow et al. - 2021 - Point-of-Interest (POI) Data Validation Methods A.pdf:application/pdf;Snapshot:C\:\\Users\\tyxis\\Zotero\\storage\\6HEADAQ2\\htm.html:text/html},
}

@inproceedings{tan_evaluating_2023,
	address = {Auckland, New Zealand},
	title = {Evaluating {Visual} {Odometry} {Methods} for {Autonomous} {Driving} in {Rain}},
	isbn = {9798350320695},
	url = {https://ieeexplore.ieee.org/document/10260549/},
	doi = {10.1109/CASE56687.2023.10260549},
	abstract = {The increasing demand for autonomous vehicles has created a need for robust navigation systems that can also operate effectively in adverse weather conditions. Visual odometry is a technique used in these navigation systems, enabling the estimation of vehicle position and motion using input from onboard cameras. However, visual odometry accuracy can be significantly impacted in challenging weather conditions, such as heavy rain, snow, or fog. In this paper, we evaluate a range of visual odometry methods, including our DROIDSLAM based heuristic approach. Specifically, these algorithms are tested on both clear and rainy weather urban driving data to evaluate their robustness. We compiled a dataset comprising of a range of rainy weather conditions from different cities. This includes, the Oxford Robotcar dataset from Oxford, the 4Seasons dataset from Munich and an internal dataset collected in Singapore. We evaluated different visual odometry algorithms for both monocular and stereo camera setups using the Absolute Trajectory Error (ATE). Our evaluation suggests that the Depth and Flow for Visual Odometry (DF-VO) algorithm with monocular setup worked well for short range distances ({\textless} 500ùëö) and our proposed DROID-SLAM based heuristic approach for the stereo setup performed relatively well for long-term localization. Both algorithms performed consistently well across all rain conditions.},
	language = {en},
	urldate = {2023-10-10},
	booktitle = {2023 {IEEE} 19th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	publisher = {IEEE},
	author = {Tan, Yu Xiang and Prasetyo, Marcel Bartholomeus and Daffa, Mohammad Alif and Nitin, Deshpande Sunny and Meghjani, Malika},
	month = aug,
	year = {2023},
	pages = {1--8},
	file = {Tan et al. - 2023 - Evaluating Visual Odometry Methods for Autonomous .pdf:C\:\\Users\\tyxis\\Zotero\\storage\\LW8Y2TY5\\Tan et al. - 2023 - Evaluating Visual Odometry Methods for Autonomous .pdf:application/pdf},
}

@misc{tan_localization_2023,
	title = {Localization with {Anticipation} for {Autonomous} {Urban} {Driving} in {Rain}},
	url = {http://arxiv.org/abs/2306.09134},
	abstract = {This paper presents a localization algorithm for autonomous urban vehicles under rain weather conditions. In adverse weather, human drivers anticipate the location of the ego-vehicle based on the control inputs they provide and surrounding road contextual information. Similarly, in our approach for localization in rain weather, we use visual data, along with a global reference path and vehicle motion model for anticipating and better estimating the pose of the ego-vehicle in each frame. The global reference path contains useful road contextual information such as the angle of turn which can be potentially used to improve the localization accuracy especially when sensors are compromised. We experimented on the Oxford Robotcar Dataset and our internal dataset from Singapore to validate our localization algorithm in both clear and rain weather conditions. Our method improves localization accuracy by 50.83\% in rain weather and 34.32\% in clear weather when compared to baseline algorithms.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Tan, Yu Xiang and Meghjani, Malika and Prasetyo, Marcel Bartholomeus},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09134 [cs]},
	keywords = {Computer Science - Robotics},
	annote = {Comment: Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023},
	file = {arXiv.org Snapshot:C\:\\Users\\tyxis\\Zotero\\storage\\9R72QH7S\\2306.html:text/html;Full Text PDF:C\:\\Users\\tyxis\\Zotero\\storage\\N6646RKJ\\Tan et al. - 2023 - Localization with Anticipation for Autonomous Urba.pdf:application/pdf},
}

@inproceedings{thengane_online_2024,
	title = {Online {Informative} {Sampling} {Using} {Semantic} {Features} in {Underwater} {Environments}},
	url = {https://ieeexplore.ieee.org/document/10682405/?arnumber=10682405},
	doi = {10.1109/OCEANS51537.2024.10682405},
	abstract = {The underwater world remains largely unexplored, with Autonomous Underwater Vehicles (AUVs) playing a crucial role in sub-sea explorations. However, continuous monitoring of underwater environments using AUV s can generate a sig-nificant amount of data. In addition, sending live data feed from an underwater environment requires dedicated on-board data storage options for AUV s which can hinder requirements of other higher priority tasks. Informative sampling techniques offer a solution by condensing observations. In this paper, we present a semantically-aware online informative sampling (ON- IS) approach which samples an AUV's visual experience in real- time. Specifically, we obtain visual features from a fine-tuned object detection model to align the sampling outcomes with the desired semantic information. Our contributions are (a) a novel Semantic Online Informative Sampling (SON-IS) algorithm, (b) a user study to validate the proposed approach and (c) a novel evaluation metric to score our proposed algorithm with respect to the suggested samples by human subjects.},
	urldate = {2025-01-03},
	booktitle = {{OCEANS} 2024 - {Singapore}},
	author = {Thengane, Shrutika Vishal and Tan, Yu Xiang and Prasetyo, Marcel Bartholomeus and Meghjani, Malika},
	month = apr,
	year = {2024},
	keywords = {Visualization, Measurement, Semantics, Object detection, Autonomous underwater vehicles, Memory, Oceans, Online Informative Sampling, Online Summa-rization, ROST, Semantic Features, Underwater Exploration, Video Summarization},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\tyxis\\Zotero\\storage\\CIQV2DT8\\Thengane et al. - 2024 - Online Informative Sampling Using Semantic Feature.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\tyxis\\Zotero\\storage\\ZGXI2372\\10682405.html:text/html},
}

@inproceedings{thengane_wavediff_2024,
	title = {{WaveDiff}: {Underwater} {Visual} {Data} {Enhancement} with {Wavelet}-{Accelerated} {Diffusion}},
	shorttitle = {{WaveDiff}},
	url = {https://ieeexplore.ieee.org/document/10753857/},
	doi = {10.1109/OCEANS55160.2024.10753857},
	abstract = {Unmanned underwater vehicles generally rely on high quality visual data and low latency for the applications of monitoring, exploration and search. Several methods have been proposed to improve underwater visibility by enhancing images taken by cameras using generative methods such as diffusion models. Although diffusion models have proven very useful for visual data enhancement, they enhance images iteratively, making this process computationally inefficient. Current methods primarily focus on improving visibility without addressing operation latency or speed, rendering these methods unsuitable for low bandwidth or fast information tracking. To overcome this, we propose using discrete wavelet transform (DWT) to decompose the image into four frequency components, reducing the spatial dimension. Out of these four components, we use only one as part of the diffusion process. At the end of the diffusion process, we apply the inverse discrete wavelet transform to obtain the enhanced image. This simple strategy helps reduce processing time by 50\% with minimal loss in Peak Signal to Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) on the LSUI dataset.},
	urldate = {2025-05-30},
	booktitle = {{OCEANS} 2024 - {Halifax}},
	author = {Thengane, Shrutika Vishal and Tan, Yu Xiang and Prasetyo, Marcel Bartholomeus and Meghjani, Malika},
	month = sep,
	year = {2024},
	note = {ISSN: 2996-1882},
	keywords = {Diffusion models, Diffusion Models, Diffusion processes, Discrete wavelet transforms, Image Enhancement, Loss measurement, PSNR, Sea measurements, Time measurement, Training, Transforms, Underwater Exploration, Visual Data Enhancement, Visualization, Wavelet Transform},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\tyxis\\Zotero\\storage\\DYIPGF6T\\Thengane et al. - 2024 - WaveDiff Underwater Visual Data Enhancement with .pdf:application/pdf},
}

@inproceedings{tan_robust_2024,
	title = {Robust {Vehicle} {Localization} and {Tracking} in {Rain} {Using} {Street} {Maps}},
	url = {https://ieeexplore.ieee.org/document/10919493/},
	doi = {10.1109/ITSC58415.2024.10919493},
	abstract = {GPS-based vehicle localization and tracking suffers from unstable positional information commonly experienced in tunnel segments and in dense urban areas. Also, both Visual Odometry (VO) and Visual Inertial Odometry (VIO) are susceptible to adverse weather conditions that causes occlusions or blur on the visual input. In this paper, we propose a novel approach for vehicle localization that uses street network based map information to correct drifting odometry estimates and intermittent GPS measurements especially, in adversarial scenarios such as driving in rain and tunnels. Specifically, our approach is a flexible fusion algorithm that integrates intermittent GPS, drifting IMU and VO estimates together with 2D map information for robust vehicle localization and tracking. We refer to our approach as Map-Fusion. We robustly evaluate our proposed approach on four geographically diverse datasets from different countries ranging across clear and rain weather conditions. These datasets also include challenging visual segments in tunnels and underpasses. We show that with the integration of the map information, our Map-Fusion algorithm reduces the error of the state-of-the-art VO and VIO approaches across all datasets. We also validate our proposed algorithm in a real-world environment and in real-time on a hardware constrained mobile robot. Map-Fusion achieved 2.46m error in clear weather and 6.05m error in rain weather for a 150 m route.},
	urldate = {2025-05-30},
	booktitle = {2024 {IEEE} 27th {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Tan, Yu Xiang and Meghjani, Malika},
	month = sep,
	year = {2024},
	note = {ISSN: 2153-0017},
	keywords = {Global Positioning System, Location awareness, Meteorology, Odometry, Rain, Real-time systems, Tracking, Trajectory, Urban areas, Visualization},
	pages = {1389--1396},
	file = {Full Text PDF:C\:\\Users\\tyxis\\Zotero\\storage\\D24ZVRYI\\Tan and Meghjani - 2024 - Robust Vehicle Localization and Tracking in Rain U.pdf:application/pdf},
}

@inproceedings{thengane_merlion_2025,
	title = {{MERLION}: {Marine} {ExploRation} with {Language} {guIded} {Online} {iNformative} {Visual} {Sampling} and {Enhancement}},
	abstract = {Autonomous and targeted underwater visual monitoring and exploration using Autonomous Underwater Vehicles (AUVs) can be a challenging task due to both online and offline constraints. The online constraints comprise limited onboard storage capacity and communication bandwidth to the surface, whereas the offline constraints entail the time and effort required for the selection of desired keyframes from the video data. An example use case of targeted underwater visual monitoring is finding the most interesting visual frames of fish in a long sequence of an AUV‚Äôs visual experience. This challenge of targeted informative sampling is further aggravated in murky waters with poor visibility. In this paper, we present MERLION, a novel framework that provides semantically aligned and visually enhanced summaries for murky underwater marine environment monitoring and exploration. Specifically, our framework integrates (a) an image-text model for semantically aligning the visual samples to the user‚Äôs needs, (b) an image enhancement model for murky water visual data and (c) an informative sampler for summarizing the monitoring experience. We validate our proposed MERLION framework on real-world data with user studies and present qualitative and quantitative results using our evaluation metric and show improved results compared to the state-of-the-art approaches. We have open-sourced the code for MERLION at the following link https://github.com/MARVL-Lab/MERLION.git.},
	language = {en},
	booktitle = {2025 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA} 2025)},
	author = {Thengane, Shrutika and Prasetyo, Marcel Bartholomeus and Tan, Yu Xiang and Meghjani, Malika},
	month = may,
	year = {2025},
	file = {Thengane et al. - MERLION Marine ExploRation with Language guIded O.pdf:C\:\\Users\\tyxis\\Zotero\\storage\\234VLHQW\\Thengane et al. - MERLION Marine ExploRation with Language guIded O.pdf:application/pdf},
}
